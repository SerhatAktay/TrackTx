// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
/* nextflow.config â€” central runtime settings & profiles */
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

nextflow.enable.dsl = 2

// â”€â”€ Dynamic resource detection
def detectCpus() {
  def cpus = Runtime.runtime.availableProcessors()
  def envCpus = System.getenv('NXF_HOST_CPUS')
  return envCpus ? (envCpus as Integer) : cpus
}

def detectMemoryGb() {
  try {
    def osBean = java.lang.management.ManagementFactory.operatingSystemMXBean
    def method = osBean.class.getMethod('getTotalPhysicalMemorySize')
    long totalBytes = (Long) method.invoke(osBean)
    return Math.max(2, (int)(totalBytes >> 30)) // GB, minimum 2GB
  } catch(Exception e) {
    def envMem = System.getenv('NXF_HOST_MEM')
    return envMem ? (envMem as Integer) : 8 // Default to 8GB
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// THROUGHPUT-OPTIMIZED RESOURCE ALLOCATION (DEFAULT)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// This system uses TOOL EFFICIENCY SCIENCE to maximize overall pipeline speed.
// Instead of giving maximum CPUs to single tasks, it allocates CPUs at each 
// tool's "sweet spot" for efficiency, then maximizes parallelism.
//
// ğŸ¯ CORE PRINCIPLE: "Efficiency Sweet Spots"
//   â€¢ Tools have optimal CPU counts where they're most efficient (usually 4-6)
//   â€¢ Beyond that: diminishing returns (wasted resources)
//   â€¢ Better to run 2 tasks at 80% efficiency than 1 task at 50% efficiency
//
// ğŸ“Š ALIGNMENT SCALING (Bowtie2) - Most Critical Process:
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚ System   â”‚ CPUs/task  â”‚ Max Forks   â”‚ Total Used   â”‚ Explanation       â”‚
// â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
// â”‚ 4 CPUs   â”‚ 4          â”‚ 1           â”‚ 4            â”‚ All-in sequential â”‚
// â”‚ 8 CPUs   â”‚ 4          â”‚ 2           â”‚ 8 âœ“          â”‚ 2Ã— parallel!      â”‚
// â”‚ 12 CPUs  â”‚ 5          â”‚ 2           â”‚ 10           â”‚ Efficient 2Ã—      â”‚
// â”‚ 16 CPUs  â”‚ 6          â”‚ 2-3         â”‚ 12-18 âœ“      â”‚ Optimal 3Ã—        â”‚
// â”‚ 24 CPUs  â”‚ 6          â”‚ 4           â”‚ 24 âœ“         â”‚ Maximum 4Ã—        â”‚
// â”‚ 32 CPUs  â”‚ 6          â”‚ 5           â”‚ 30           â”‚ 5Ã— parallel       â”‚
// â”‚ 64 CPUs  â”‚ 8          â”‚ 8           â”‚ 64 âœ“         â”‚ 8Ã— parallel       â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
//
// ğŸ’¡ WHY THIS IS BETTER:
// â€¢ Bowtie2 at 4 CPUs (88% efficient) Ã— 2 tasks = 7.0Ã— total speedup
// â€¢ Bowtie2 at 8 CPUs (72% efficient) Ã— 1 task = 5.8Ã— total speedup
// â†’ Parallel approach is 20% faster overall!
//
// ğŸš€ EXPECTED SPEEDUP vs OLD SEQUENTIAL APPROACH:
// â€¢ 4 CPUs:   ~Same (no room for parallelism)
// â€¢ 8 CPUs:   30-50% faster (2Ã— parallel at efficiency peak)
// â€¢ 12 CPUs:  40-70% faster (2-3Ã— parallel)
// â€¢ 16+ CPUs: 80-150% faster (3-4Ã— parallel with high efficiency)

// Core system detection
def getSystemCpus() {
  def available = detectCpus()
  return Math.max(1, available)  // Always have at least 1 CPU
}

def getSystemMemoryGb() {
  def availableGb = detectMemoryGb()
  return Math.max(2, availableGb)  // Always have at least 2GB
}

// â”€â”€ THROUGHPUT-OPTIMIZED CPU ALLOCATION (based on tool efficiency) â”€â”€â”€â”€â”€â”€â”€â”€

// Bowtie2 alignment: Optimal = 4-6 CPUs, Max useful = 8 CPUs
def alignmentCpus() {
  def total = getSystemCpus()
  
  if (total <= 4) return total           // 4 CPUs: use all (1 task sequential)
  if (total <= 6) return 4               // 6 CPUs: 4 per task
  if (total <= 8) return 4               // 8 CPUs: 4 per task (2 tasks parallel!)
  if (total <= 12) return 5              // 12 CPUs: 5 per task (2 tasks parallel)
  if (total <= 16) return 6              // 16 CPUs: 6 per task (2-3 tasks parallel)
  if (total <= 24) return 6              // 24 CPUs: 6 per task (4 tasks parallel)
  if (total <= 32) return 6              // 32 CPUs: 6 per task (5 tasks parallel)
  return 8                               // 32+ CPUs: 8 per task (max efficiency)
}

def alignmentForks() {
  def total = getSystemCpus()
  def perTask = alignmentCpus()
  
  if (total < 8) return 1                // <8 CPUs: sequential
  return Math.max(1, (int)(total / perTask))  // Fit as many tasks as possible
}

// Cutadapt (prepare_input): Optimal = 4 CPUs, Max useful = 6 CPUs
def prepareCpus() {
  def total = getSystemCpus()
  
  if (total <= 4) return Math.max(2, total - 1)  // Leave 1 CPU free on tiny systems
  if (total <= 8) return 4               // 8 CPUs: 4 per task (2 parallel)
  if (total <= 12) return 4              // 12 CPUs: 4 per task (3 parallel)
  if (total <= 16) return 4              // 16 CPUs: 4 per task (4 parallel)
  return 4                               // Any size: stick to efficiency peak
}

def prepareForks() {
  def total = getSystemCpus()
  def perTask = prepareCpus()
  
  if (total < 6) return 1                // <6 CPUs: sequential
  return Math.max(1, (int)(total / perTask))
}

// Samtools/bedtools (track generation): Optimal = 3-4 CPUs, Max useful = 6 CPUs
def tracksCpus() {
  def total = getSystemCpus()
  
  if (total <= 4) return Math.max(2, total - 1)  // Small systems: leave 1 CPU
  if (total <= 8) return 3               // 8 CPUs: 3 per task (2-3 parallel)
  if (total <= 12) return 4              // 12 CPUs: 4 per task (3 parallel)
  if (total <= 16) return 4              // 16 CPUs: 4 per task (4 parallel)
  return 4                               // Any size: stick to sweet spot
}

def tracksForks() {
  def total = getSystemCpus()
  def perTask = tracksCpus()
  
  if (total < 6) return 1
  return Math.max(1, (int)(total / perTask))
}

// Normalization: CPU + memory intensive, optimal = 6-8 CPUs
def normCpus() {
  def total = getSystemCpus()
  
  if (total <= 4) return total           // Use all on tiny systems
  if (total <= 8) return 6               // 8 CPUs: 6 per task (1 task, save some)
  if (total <= 12) return 6              // 12 CPUs: 6 per task (2 parallel)
  if (total <= 16) return 6              // 16 CPUs: 6 per task (2 parallel) 
  if (total <= 24) return 8              // 24 CPUs: 8 per task (3 parallel)
  return 8                               // Large: 8 per task
}

def normForks() {
  def total = getSystemCpus()
  def perTask = normCpus()
  
  if (total < 10) return 1               // <10 CPUs: sequential (memory intensive!)
  return Math.max(1, Math.min(3, (int)(total / perTask)))  // Max 3 parallel (memory limit)
}

// Python multiprocessing (divergent): Optimal = 6-8 CPUs, Max useful = 12 CPUs
def divergentCpus() {
  def total = getSystemCpus()
  
  if (total <= 4) return total           // Use all
  if (total <= 8) return 6               // 8 CPUs: 6 per task
  if (total <= 12) return 6              // 12 CPUs: 6 per task (2 parallel)
  if (total <= 16) return 8              // 16 CPUs: 8 per task (2 parallel)
  if (total <= 24) return 8              // 24 CPUs: 8 per task (3 parallel)
  return 8                               // Large: 8 per task (sweet spot)
}

def divergentForks() {
  def total = getSystemCpus()
  def perTask = divergentCpus()
  
  if (total < 12) return 1               // <12 CPUs: sequential (memory intensive!)
  return Math.max(1, Math.min(3, (int)(total / perTask)))  // Max 3 parallel (memory limit)
}

// Index building: One-time task, can be generous
def indexCpus() {
  def total = getSystemCpus()
  
  if (total <= 4) return total
  if (total <= 8) return 6
  if (total <= 16) return 8
  if (total <= 32) return 12
  return 16                              // Very large systems can use more
}

// Analysis tasks: Lightweight, maximize parallelism
def analysisCpus() {
  def total = getSystemCpus()
  
  if (total <= 4) return 2
  if (total <= 8) return 2
  if (total <= 16) return 3
  return 4
}

def analysisForks() {
  def total = getSystemCpus()
  def perTask = analysisCpus()
  
  return Math.max(2, (int)(total / perTask))  // High parallelism
}

// Lightweight tasks (downloads, reports)
def lightweightCpus() {
  return 1  // Single-threaded
}

// â”€â”€ INTELLIGENT MEMORY ALLOCATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// High-intensity memory allocation (alignment, normalization)
def intensiveMemory(baseGb = 8) {
  def totalGb = getSystemMemoryGb()
  if (totalGb < baseGb) {
    return "${(int)(totalGb * 0.8)} GB"  // Use 80% if below base requirement
  }
  // Scale memory allocation: more memory = higher allocation up to 70%
  def allocated = Math.min((baseGb * 2) as int, (int)(totalGb * 0.7))
  return "${allocated} GB"
}

// Moderate memory allocation (track generation, analysis)
def moderateMemory(baseGb = 4) {
  def totalGb = getSystemMemoryGb()
  def allocated = Math.min((baseGb * 1.5) as int, (int)(totalGb * 0.4))
  return "${Math.max(2 as int, allocated as int)} GB"
}

// Light memory allocation (downloads, reports)
def lightMemory() {
  def totalGb = getSystemMemoryGb()
  def allocated = Math.max(1 as int, (int)(totalGb * 0.1))
  return "${Math.min(4 as int, allocated as int)} GB"
}

// Legacy compatibility (if any old code references these)
def adaptiveCpus(preferredCpus) {
  return prepareCpus()  // Use throughput approach
}

def adaptiveMemory(preferredGb) {
  return moderateMemory(preferredGb)
}

// Legacy fullPowerCpus/balancedCpus (for backward compatibility with old configs)
def fullPowerCpus(min, max) {
  return alignmentCpus()  // Use throughput approach
}

def balancedCpus(preferred, max) {
  return tracksCpus()  // Use throughput approach
}

// â”€â”€ Set verbosity (change to 'debug' for more verbose output)
log.level = 'debug'

// â”€â”€ Manifest (metadata + required Nextflow version)
manifest {
  name            = 'tracktx-nf'
  author          = 'Serhat Aktay <serhat.aktay@scilifelab.se>'
  description     = 'Nextflow pipeline for nascent RNA analysis (TrackTx-NF)'
  homePage        = 'https://github.com/serhataktay/tracktx-nf'
  version         = '1.0.0'
  mainScript      = 'main.nf'
  nextflowVersion = '>=24.04.0'
}

// â”€â”€ Plugins & schema (nf-schema v2.x) - DISABLED due to filesystem symlink issues
// plugins {
//   id 'nf-schema@2.4.1'
// }

// nf-schema config (path is relative to project root) - DISABLED
// validation {
//   parametersSchema = 'assets/nextflow_schema.json'
//   help {
//     enabled = true
//     command = 'nextflow run main.nf -entry TrackTx -profile docker -with-report -with-timeline -with-dag'
//   }
// }

// â”€â”€ Default params file will be loaded via -params-file in main.nf

// â”€â”€ Global params (pure assignments)
params {
  // Output
  output_dir = './results'

  // Core parameters (define early to prevent warnings)
  help = false
  debug = false
  debug_report = false

  // Parameter objects (empty by default)
  advanced = [:]
  adapter_trimming = [:]
  barcode = [:]
  umi = [:]

  // Index/cache controls
  assets_dir    = "${projectDir}/assets"
  genome_cache  = "${projectDir}/.cache/genomes"
  force_rebuild = false

  // Reporting/plots toggle (0/1); piped to reports module via env
  reports_plots = 0

  // Pol II aggregate contrasts (e.g. ["Heat:CTRL","KI:WT"])
  pol2 {
    contrasts = []
  }

  // QC & misc toggles
  qc = [:]
  conda_pol2   = null
  debug        = false
  debug_report = false

  conda_norm = null
  conda_divergent = null
  conda_fgr = null
  conda_sra = null
  gtf_path  = null
  gtf_url   = null

  // Defaults for common toggles (users can override via params.yaml/CLI)
  fastqc_raw               = true
  counts_allow_index_build = true
  force_sort_bedgraph      = false
  paired_end               = false
}

// (Note: defaults above can be overridden by -params-file or CLI)

// â”€â”€ Pretty logs
ansi { log = true }

// â”€â”€ Global process defaults with resource allocation
process {
  executor  = 'local'
  withConda = true      // ignored when a container is set
  scratch   = true
  cleanup   = true

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // Default Resource Allocation (Throughput-Optimized)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  // DEFAULT: Lightweight tasks (most processes are overridden below)
  cpus          = { lightweightCpus() }    // Single-threaded by default
  memory        = moderateMemory(3)        // Auto-scale memory based on available RAM
  
  // Time limit: 24 hours (generous for most tasks)
  time          = '24h'
  
  // Error handling: Retry on memory/timeout errors, terminate on others
  errorStrategy = { task.exitStatus in [137,143] ? 'retry' : 'terminate' }
  maxRetries    = 1
  maxErrors     = -1

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // Environment Variables for Optimal Threading
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  beforeScript = {
    """
    # Set threading environment variables based on allocated CPUs
    export OMP_NUM_THREADS=${task.cpus}
    export OPENBLAS_NUM_THREADS=${task.cpus}
    export MKL_NUM_THREADS=${task.cpus}
    export BOWTIE2_THREADS=${task.cpus}
    export SAMTOOLS_THREADS=${task.cpus}
    export BEDTOOLS_THREADS=${task.cpus}
    """
  }

  // Docker container resource limits and labels  
  containerOptions = { 
    "--cpus=${task.cpus} --label nf-task-hash=${task.hash} --label nf-process=${task.process}" 
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // Process-Specific Resource Allocations
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  // â•â•â• HIGH-INTENSITY PROCESSES (Throughput-Optimized) â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  // Alignment: Most CPU-intensive - use efficiency sweet spot + parallelism
  withName: /(?i).*run_alignment.*/ {
    cpus   = { alignmentCpus() }      // Adaptive: 4-8 CPUs based on system size
    memory = intensiveMemory(10)      // High memory for BAM processing
    time   = '8h'
    maxForks = alignmentForks()       // Enable parallelism on 8+ CPU systems
  }

  // Index Building: One-time task, can use more resources
  withName: /(?i).*fetch_and_build_index.*/ {
    cpus   = { indexCpus() }          // Adaptive: 4-16 CPUs based on system
    memory = intensiveMemory(8)       // High memory for large genomes
    time   = '6h'
    maxForks = 1                      // Only one index at a time
  }

  // â•â•â• MEDIUM-INTENSITY PROCESSES (Throughput-Optimized) â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  // Track Generation: Samtools/bedtools processing
  withName: /(?i).*generate_tracks.*/ {
    cpus   = { tracksCpus() }         // Adaptive: 3-4 CPUs (efficiency sweet spot)
    memory = moderateMemory(5)        // Moderate memory needs
    time   = '6h'
    maxForks = tracksForks()          // Enable parallelism based on system size
  }

  // Track Normalization: BigWig conversion (memory + CPU intensive)
  withName: /(?i).*normalize_tracks.*/ {
    cpus   = { normCpus() }           // Adaptive: 6-8 CPUs per task
    memory = intensiveMemory(10)      // High memory for BigWig conversions
    time   = '8h'
    maxForks = normForks()            // Controlled parallelism (memory limits)
    scratch = true                    // Use scratch space for I/O
  }

  // Divergent Detection: Python multiprocessing
  withName: /(?i).*detect_divergent_tx.*/ {
    cpus         = { divergentCpus() } // Adaptive: 6-8 CPUs (good scaling)
    memory       = intensiveMemory(10) // High memory for chromosome processing
    time         = '8h'
    maxForks     = divergentForks()     // Controlled parallelism (memory limits)
    errorStrategy = { task.exitStatus in [137,143] ? 'retry' : 'terminate' }
    maxRetries    = 2
    scratch       = true
  }

  // â•â•â• MODERATE INTENSITY PROCESSES â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  // Input Preparation: Cutadapt + FastQC
  withName: /(?i).*prepare_input.*/ {
    cpus   = { prepareCpus() }        // Adaptive: 4 CPUs (efficiency peak)
    memory = moderateMemory(5)        // Memory for FASTQ files
    time   = '4h'
    maxForks = prepareForks()         // Enable parallelism based on system
  }

  // POL2 Metrics & Functional Regions: Analysis tasks
  withName: /(?i).*calculate_pol2_metrics.*/ {
    cpus   = { analysisCpus() }       // Adaptive: 2-4 CPUs
    memory = moderateMemory(5)        // Memory for BAM processing
    time   = '4h'
    maxForks = analysisForks()        // High parallelism
  }
  
  withName: /(?i).*call_functional_regions.*/ {
    cpus   = { analysisCpus() }       // Adaptive: 2-4 CPUs
    memory = moderateMemory(4)        // Moderate memory
    time   = '4h'
    maxForks = analysisForks()        // High parallelism
  }

  // Count Collection & QC: Very lightweight
  withName: /(?i)(collect_counts|qc_pol2_tracktx|summarize_pol2_metrics).*/ {
    cpus   = { Math.min(2, Math.max(1, (int)(getSystemCpus() / 4))) }
    memory = moderateMemory(3)
    time   = '2h'
    maxForks = Math.max(4, (int)(getSystemCpus() / 2))      // Maximum parallelism
  }

  // â•â•â• LIGHTWEIGHT PROCESSES â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  // GTF Download: Network-bound, minimal CPU
  withName: /(?i).*download_gtf.*/ {
    cpus   = { lightweightCpus() }    // Single-threaded
    memory = lightMemory()            // Minimal memory
    time   = '4h'                     // Extra time for network issues
  }

  // SRR Download: Network + fasterq-dump conversion benefits from 2-4 CPUs
  withName: /(?i).*download_srr.*/ {
    cpus   = { Math.min(4, Math.max(2, (int)(getSystemCpus() / 4))) }  // 2-4 CPUs for fasterq-dump
    memory = moderateMemory(4)        // More memory for SRA conversion
    time   = '6h'                     // Extra time for large datasets
    maxForks = Math.max(2, (int)(getSystemCpus() / 4))  // Allow parallel downloads
  }

  // Reports: Simple text processing
  withName: /(?i)(generate_reports|combine_reports).*/ {
    cpus   = { lightweightCpus() }    // Single-threaded
    memory = lightMemory()            // Minimal memory
    time   = '1h'
  }

  // Pass plots toggle to the reports module
  withName: generate_reports {
    env.REPORTS_PLOTS = "${params.reports_plots}"
  }
}

// â”€â”€ Tracing (fixed filenames; Nextflow creates parent dirs automatically)
def TRACE_DIR = "${params.output_dir}/trace"

trace {
  enabled   = true
  file      = "${TRACE_DIR}/trace.txt"
  sep       = '\t'
  fields    = 'task_id,hash,process,tag,cpus,memory,%cpu,%mem,rss,peak_rss,vmem,peak_vmem,realtime,duration,read_bytes,write_bytes,attempt,workdir'
  overwrite = true
}

report {
  enabled   = true
  file      = "${TRACE_DIR}/report.html"
  overwrite = true
}

timeline {
  enabled   = true
  file      = "${TRACE_DIR}/timeline.html"
  overwrite = true
}

// Optional but nice to have: static DAG image alongside other artifacts
dag {
  enabled   = true
  file      = "${TRACE_DIR}/flow_dag.png"
  overwrite = true
}

// â”€â”€ Profiles (choose one or layer: e.g., -profile slurm,docker)
profiles {

  // Local + Conda (laptop / small server)
  conda {
    process.executor  = 'local'
    conda.enabled     = true
    process.conda     = "${projectDir}/envs/tracktx.yaml"
    process.withConda = true
    
    // Conda configuration (flattened to avoid nested block conflicts)
    conda.verbose = true
    conda.showChannelUrls = true
    conda.useMamba = false  // Set to true if mamba is installed for faster solves
    
    // Ensure conda environment is properly activated
    // Respect NXF_CONDA_CACHEDIR if set, else default to repo-local cache
    conda.cacheDir    = "${ System.getenv('NXF_CONDA_CACHEDIR') ?: projectDir + '/.conda' }"
    conda.createTimeout = '2h'  // Increased timeout for slower network storage
    conda.createOptions = '--no-channel-priority --repodata-fn=repodata.json'

    // Enhanced NFS-safe defaults to avoid hardlink/symlink and locking issues
    process {
      withConda = true
      // Core conda/mamba safety settings for network storage
      env.CONDA_ALWAYS_COPY      = "${ System.getenv('CONDA_ALWAYS_COPY') ?: '1' }"
      env.CONDA_COPY_ALWAYS      = "${ System.getenv('CONDA_COPY_ALWAYS') ?: '1' }"
      env.MAMBA_ALLOW_HARDLINKS  = "${ System.getenv('MAMBA_ALLOW_HARDLINKS') ?: '0' }"
      env.MAMBA_ALLOW_SYMLINKS   = "${ System.getenv('MAMBA_ALLOW_SYMLINKS')  ?: '0' }"
      env.MAMBA_EXTRACT_THREADS  = "${ System.getenv('MAMBA_EXTRACT_THREADS') ?: '1' }"
      env.MAMBA_NO_LOCK          = "${ System.getenv('MAMBA_NO_LOCK') ?: '1' }"
      env.MAMBA_NO_HARDLINKS     = "${ System.getenv('MAMBA_NO_HARDLINKS') ?: '1' }"
      
      // Disable problematic conda features for network storage
      env.CONDA_USE_ONLY_TAR_BZ2 = "${ System.getenv('CONDA_USE_ONLY_TAR_BZ2') ?: '0' }"
      env.CONDA_SAFETY_CHECKS    = "${ System.getenv('CONDA_SAFETY_CHECKS') ?: 'disabled' }"
      env.CONDA_EXTRA_SAFETY_CHECKS = "${ System.getenv('CONDA_EXTRA_SAFETY_CHECKS') ?: 'no' }"
      
      // Retry and timeout settings
      env.CONDA_REMOTE_CONNECT_TIMEOUT_SECS = "${ System.getenv('CONDA_REMOTE_CONNECT_TIMEOUT_SECS') ?: '60' }"
      env.CONDA_REMOTE_READ_TIMEOUT_SECS    = "${ System.getenv('CONDA_REMOTE_READ_TIMEOUT_SECS') ?: '300' }"
      env.CONDA_REMOTE_MAX_RETRIES          = "${ System.getenv('CONDA_REMOTE_MAX_RETRIES') ?: '5' }"
      
      // Archive handling for network storage
      env.CONDA_VERIFY_SSL       = "${ System.getenv('CONDA_VERIFY_SSL') ?: 'true' }"
      env.CONDA_CHANNEL_PRIORITY = "${ System.getenv('CONDA_CHANNEL_PRIORITY') ?: 'flexible' }"
      
      // Allow user override of PKGS dir; otherwise conda chooses under cacheDir
      if( System.getenv('CONDA_PKGS_DIRS') ) {
        env.CONDA_PKGS_DIRS = System.getenv('CONDA_PKGS_DIRS')
      }
    }
  }

  // Server-optimized conda (robust fallback for non-Docker environments)
  conda_server {
    process.executor  = 'local'
    conda.enabled     = true
    process.conda     = "${projectDir}/envs/tracktx.yaml"
    process.withConda = true
    
    // Ultra-conservative settings for problematic servers (flattened)
    conda.verbose = true
    conda.showChannelUrls = true
    conda.useMamba = false  // Disable mamba if causing issues, fall back to conda
    conda.createTimeout = '4h'  // Very long timeout for slow servers
    conda.createOptions = '--no-channel-priority --repodata-fn=repodata.json --no-deps --force-reinstall'
    
    // Force local cache to avoid network storage issues
    conda.cacheDir = "${ System.getenv('TMPDIR') ?: '/tmp' }/nextflow-conda-${System.getProperty('user.name')}"
    
    // Maximum safety settings for server environments
    process {
      withConda = true
      
      // Absolutely no file operations that can fail on network storage
      env.CONDA_ALWAYS_COPY         = '1'
      env.CONDA_COPY_ALWAYS         = '1'
      env.MAMBA_ALLOW_HARDLINKS     = '0'
      env.MAMBA_ALLOW_SYMLINKS      = '0'
      env.MAMBA_EXTRACT_THREADS     = '1'
      env.MAMBA_NO_LOCK             = '1'
      env.MAMBA_NO_HARDLINKS        = '1'
      env.MAMBA_NO_BANNER           = '1'
      
      // Disable all conda safety checks that can cause issues
      env.CONDA_SAFETY_CHECKS       = 'disabled'
      env.CONDA_EXTRA_SAFETY_CHECKS = 'no'
      env.CONDA_USE_ONLY_TAR_BZ2    = '1'  // Use older, more reliable format
      env.CONDA_CHANNEL_PRIORITY    = 'disabled'
      
      // Extended timeouts and retries
      env.CONDA_REMOTE_CONNECT_TIMEOUT_SECS = '120'
      env.CONDA_REMOTE_READ_TIMEOUT_SECS    = '600'
      env.CONDA_REMOTE_MAX_RETRIES          = '10'
      env.CONDA_REMOTE_BACKOFF_FACTOR       = '2'
      
      // Force package cache to local temp directory
      env.CONDA_PKGS_DIRS = "${ System.getenv('TMPDIR') ?: '/tmp' }/conda-pkgs-${System.getProperty('user.name')}"
      
      // Solver settings for reliability over speed
      env.CONDA_SOLVER = 'classic'  // Use classic solver, not libmamba
      env.CONDA_EXPERIMENTAL = 'no'
      
      // Memory and resource limits
      env.CONDA_REPODATA_THREADS = '1'
      env.CONDA_FETCH_THREADS = '1'
    }
    
    // Pre-execution cleanup and validation
    beforeScript = '''
      # Ensure cache directories exist and are writable
      mkdir -p "${CONDA_PKGS_DIRS}" || true
      chmod 755 "${CONDA_PKGS_DIRS}" || true
      
      # Clean any stale lock files
      find "${CONDA_PKGS_DIRS}" -name "*.lock" -mtime +1 -delete 2>/dev/null || true
      
      # Test write access
      touch "${CONDA_PKGS_DIRS}/.write_test" && rm -f "${CONDA_PKGS_DIRS}/.write_test" || {
        echo "WARNING: Cannot write to conda cache directory ${CONDA_PKGS_DIRS}"
        export CONDA_PKGS_DIRS="/tmp/conda-emergency-${RANDOM}"
        mkdir -p "${CONDA_PKGS_DIRS}"
      }
    '''
  }

  // Micromamba (faster, more reliable conda alternative for servers)
  micromamba {
    process.executor = 'local'
    conda.enabled = false  // Disable conda, use micromamba instead
    
    process {
      beforeScript = '''
        # Use bundled micromamba if available, otherwise try system micromamba
        if [[ -f "${projectDir}/bin/micromamba" ]]; then
          export MAMBA_EXE="${projectDir}/bin/micromamba"
        elif command -v micromamba >/dev/null 2>&1; then
          export MAMBA_EXE="micromamba"
        else
          echo "ERROR: micromamba not found. Please install micromamba or use conda profile."
          exit 1
        fi
        
        # Set up micromamba environment
        export MAMBA_ROOT_PREFIX="${TMPDIR:-/tmp}/micromamba-${USER}"
        export CONDA_PREFIX="${MAMBA_ROOT_PREFIX}/envs/tracktx"
        mkdir -p "${MAMBA_ROOT_PREFIX}"
        
        # Create environment if it doesn't exist
        if [[ ! -d "${CONDA_PREFIX}" ]]; then
          echo "Creating micromamba environment..."
          ${MAMBA_EXE} create -f envs/tracktx.yaml -p "${CONDA_PREFIX}" -y
        fi
        
        # Activate environment
        source "${MAMBA_ROOT_PREFIX}/etc/profile.d/micromamba.sh"
        micromamba activate "${CONDA_PREFIX}"
      '''
      
      env.MAMBA_NO_BANNER = '1'
      env.MAMBA_EXTRACT_THREADS = '1'
      env.MAMBA_ALLOW_HARDLINKS = '0'
    }
  }

  // Docker (recommended)
  docker {
    docker.enabled    = true
    docker.autoPull   = true
    process.container = 'ghcr.io/serhataktay/tracktx:latest'
    process.withConda = false
    docker.runOptions = '--platform=linux/amd64'   // stable on Apple Silicon
  }

  // Slurm (layer with docker or singularity: -profile slurm,docker)
  slurm {
    process.executor = 'slurm'
  }

  // Singularity/Apptainer (pull from Docker registry)
  singularity {
    singularity.enabled   = true
    singularity.cacheDir  = "${baseDir}/.singularity"
    singularity.autoMounts = true
    process.container     = 'docker://ghcr.io/serhataktay/tracktx:latest'
    process.withConda     = false
    // HPC-optimized singularity options
    singularity.runOptions = [
      '--bind /tmp:/tmp',
      '--bind /var/tmp:/var/tmp',
      '--bind /scratch:/scratch',
      '--bind /tmp/nextflow-work:/tmp/nextflow-work',
      '--cleanenv',
      '--no-home'
    ]
    // Keep executor from another layered profile (e.g., slurm)
  }

  // Apptainer (newer Singularity fork)
  apptainer {
    singularity.enabled   = true
    singularity.cacheDir  = "${baseDir}/.singularity"
    singularity.autoMounts = true
    process.container     = 'docker://ghcr.io/serhataktay/tracktx:latest'
    process.withConda     = false
    // HPC-optimized apptainer options
    singularity.runOptions = [
      '--bind /tmp:/tmp',
      '--bind /var/tmp:/var/tmp',
      '--bind /scratch:/scratch',
      '--bind /tmp/nextflow-work:/tmp/nextflow-work',
      '--cleanenv',
      '--no-home'
    ]
    // Keep executor from another layered profile (e.g., slurm)
  }

  // Podman (rootless)
  podman {
    podman.enabled    = true
    podman.autoPull   = true
    process.container = 'ghcr.io/serhataktay/tracktx:latest'
    process.withConda = false
  }
  
  // Optimized (layer with docker or singularity: -profile docker,optimized)
  // Uses MORE aggressive resource allocation (80-90% of system)
  // Only for dedicated compute nodes where you can monopolize resources
  optimized {
    includeConfig 'conf/optimized.config'
  }

  // Throughput profile (kept for backward compatibility)
  // NOTE: Throughput strategy is now THE DEFAULT! No need to specify this.
  // This profile is identical to default - included only for compatibility
  throughput {
    includeConfig 'conf/throughput.config'
  }
}
