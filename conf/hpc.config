// ════════════════════════════════════════════════════════════════════════════
/* HPC Configuration - Robust settings for cluster deployments */
// ════════════════════════════════════════════════════════════════════════════
// This configuration prevents common HPC issues and provides robust defaults
// for SLURM, PBS/Torque, and SGE schedulers.

// Shared HPC settings that work across different schedulers
process {
  // HPC-optimized resource defaults (conservative to prevent job failures)
  cpus    = 8
  memory  = '32 GB'
  time    = '48h'
  
  // Robust error handling for HPC environments
  errorStrategy = { 
    // Memory/time limits, killed jobs, or SIGTERM from scheduler
    if (task.exitStatus in [137, 143, 130, 104, 134, 139]) {
      return 'retry'
    }
    // Node failures or network issues  
    else if (task.exitStatus in [1, 2, 130]) {
      return task.attempt < 3 ? 'retry' : 'finish'
    }
    // Other failures
    else {
      return 'finish'
    }
  }
  maxRetries    = 3
  maxErrors     = 5
  
  // HPC storage best practices
  scratch = true   // Use node-local scratch when available
  cleanup = true   // Clean up to prevent quota issues
  
  // Comprehensive HPC environment setup
  beforeScript = '''
    # ═══════════════════════════════════════════════════════════════════════════
    # HPC Environment Setup - Prevents Common Cluster Issues
    # ═══════════════════════════════════════════════════════════════════════════
    
    set -euo pipefail  # Fail fast on errors
    
    # ── Scratch and Temp Directory Management ─────────────────────────────────
    # Use fast local storage when available
    if [[ -n "${SLURM_JOB_ID:-}" && -d "/tmp" ]]; then
      export TMPDIR="/tmp/nf_tmp_${SLURM_JOB_ID}_$$"
    elif [[ -n "${PBS_JOBID:-}" && -d "/scratch" ]]; then
      export TMPDIR="/scratch/nf_tmp_${PBS_JOBID}_$$"
    elif [[ -n "${JOB_ID:-}" && -d "/tmp" ]]; then
      export TMPDIR="/tmp/nf_tmp_${JOB_ID}_$$"
    else
      export TMPDIR="${TMPDIR:-/tmp}/nf_tmp_$$"
    fi
    
    mkdir -p "$TMPDIR" && chmod 700 "$TMPDIR"
    echo "INFO: Using TMPDIR=$TMPDIR"
    
    # Cleanup temp on exit
    trap 'rm -rf "$TMPDIR" 2>/dev/null || true' EXIT
    
    # ── Thread and Resource Control ───────────────────────────────────────────
    # Prevent tools from over-subscribing CPU cores
    export OMP_NUM_THREADS=${task.cpus}
    export OPENBLAS_NUM_THREADS=${task.cpus}
    export MKL_NUM_THREADS=${task.cpus}
    export NUMEXPR_NUM_THREADS=${task.cpus}
    export VECLIB_MAXIMUM_THREADS=${task.cpus}
    
    # Tool-specific threading limits
    export BOWTIE2_THREADS=${task.cpus}
    export SAMTOOLS_THREADS=${task.cpus}
    export BEDTOOLS_THREADS=${task.cpus}
    export PIGZ_THREADS=${task.cpus}
    
    # ── Memory Management ─────────────────────────────────────────────────────
    # Help tools respect memory limits
    export MALLOC_ARENA_MAX=4  # Reduce memory fragmentation
    
    # Java heap size for tools that use Java
    JAVA_MEM_GB=$(echo ${task.memory} | sed 's/[^0-9]*//g')
    if [[ -n "$JAVA_MEM_GB" && "$JAVA_MEM_GB" -gt 0 ]]; then
      JAVA_HEAP=$((JAVA_MEM_GB - 2))  # Leave 2GB for system
      export JAVA_OPTS="-Xmx${JAVA_HEAP}g -XX:+UseG1GC -XX:+UseStringDeduplication"
    fi
    
    # ── Network and I/O Optimization ─────────────────────────────────────────
    # Reduce network timeouts for flaky cluster networks
    export CURL_CA_BUNDLE="${CURL_CA_BUNDLE:-/etc/ssl/certs/ca-bundle.crt}"
    export REQUESTS_CA_BUNDLE="${CURL_CA_BUNDLE}"
    
    # ── Prevent Common HPC Tool Issues ───────────────────────────────────────
    # Clear problematic environment variables
    unset LD_PRELOAD 2>/dev/null || true
    
    # Ensure critical tools are available
    for tool in tar gzip gunzip sort awk sed grep; do
      if ! command -v "$tool" >/dev/null 2>&1; then
        echo "ERROR: Critical tool missing: $tool" >&2
        echo "HINT: Load required modules or contact system administrator" >&2
        exit 1
      fi
    done
    
    # ── File System Considerations ────────────────────────────────────────────
    # Set conservative file creation mask for shared filesystems
    umask 002
    
    # Test write permissions
    if ! touch "${PWD}/.write_test_$$" 2>/dev/null; then
      echo "ERROR: Cannot write to current directory: ${PWD}" >&2
      exit 1
    fi
    rm -f "${PWD}/.write_test_$$"
    
    echo "INFO: HPC environment setup completed successfully"
  '''
}

// Override resources using the dynamic allocation from base.config
// Conservative HPC settings to prevent job failures
params {
  advanced {
    // Memory-intensive processes - conservative allocations
    index_cpus = 16
    index_mem  = '64 GB'
    index_time = '72h'
    
    // CPU-intensive alignment - balanced for HPC
    align_cpus = 12  
    align_mem  = '48 GB'
    align_time = '48h'
    
    // Track generation - moderate resources
    tracks_cpus = 8
    tracks_mem  = '32 GB'
    tracks_time = '24h'
    
    // Normalization - CPU intensive but memory efficient
    norm_cpus = 12
    norm_mem  = '32 GB'
    norm_time = '24h'
    
    // Python analysis - memory intensive
    div_cpus = 8
    div_mem  = '24 GB'
    div_time = '24h'
    
    // Lightweight analysis processes
    light_cpus = 4
    light_mem  = '16 GB'
    light_time = '12h'
    
    // Report generation - minimal resources
    report_cpus = 2
    report_mem  = '8 GB'
    report_time = '6h'
  }
}

// HPC storage configuration
params {
  // Default to shared filesystem paths
  work_dir = "${params.work_dir ?: '/tmp/nextflow-work'}"
  output_dir = "${params.output_dir ?: './results'}"
  
  // HPC-friendly asset caching
  assets_dir = "${params.assets_dir ?: '/shared/tracktx-assets'}"
  genome_cache = "${params.genome_cache ?: '/shared/tracktx-genomes'}"
}

// HPC execution settings
executor {
  // Common HPC executor settings
  queueSize = 100
  submitRateLimit = '10/1min'
  
  // Job monitoring
  jobName = 'tracktx.{hash}'
  queueStatInterval = '30s'
  
  // Cleanup on exit
  exitOnError = false
}
