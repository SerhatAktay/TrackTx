// ════════════════════════════════════════════════════════════════════════════
/* Host Resource Auto-Detection and Dynamic Allocation */
// ════════════════════════════════════════════════════════════════════════════
// This section automatically detects your system's CPU and memory capabilities
// and dynamically allocates resources to pipeline processes for optimal performance.
// You can override these settings by setting NXF_HOST_CPUS and NXF_HOST_MEM environment variables.
def readIntEnv = { k, defv -> System.getenv(k) ? (System.getenv(k) as Integer) : defv }

def detectedCpus = readIntEnv('NXF_HOST_CPUS', Runtime.runtime.availableProcessors())
if( detectedCpus < 1 ) detectedCpus = 1

// Try to get total RAM in bytes via JMX; fall back to 16 GiB
def osBean = java.lang.management.ManagementFactory.operatingSystemMXBean
long totalBytes = 0L
try {
  def m = osBean.class.getMethod('getTotalPhysicalMemorySize')
  totalBytes = (Long) m.invoke(osBean)
} catch(Throwable ignored) { }
def detectedGb = totalBytes ? (int)(totalBytes >> 30) : 16
def envMemGb   = readIntEnv('NXF_HOST_MEM', detectedGb)
if( envMemGb < 2 ) envMemGb = 2

// Respect user-provided params.* if set (CLI/params.yaml win)
params.host_cpus = (params.host_cpus ?: detectedCpus) as Integer
params.host_mem  = (params.host_mem  ?: envMemGb   ) as Integer  // GB

// ════════════════════════════════════════════════════════════════════════════
/* Resource Allocation Helpers */
// ════════════════════════════════════════════════════════════════════════════

// Unified output directory (ensures consistency across all modules)
def OUT = params.output_dir ?: 'results'

// Helper function to allocate CPUs as a fraction of total available CPUs
// Usage: fitCpus(0.5) = use 50% of available CPUs, minimum 1
def fitCpus = { frac ->
  int hc = (params.host_cpus as int)
  int v  = Math.max(1, (int)Math.floor(hc * frac))
  return Math.min(hc, v)
}

// Helper function for dynamic memory allocation with scaling
// Usage: fitMemory(0.3, attempt) = use 30% of RAM, scale by attempt number
def fitMemory = { frac, attempt = 1 ->
  int hostGb = (params.host_mem as int)
  int baseGb = Math.max(2, (int)Math.floor(hostGb * frac))
  int scaledGb = baseGb * (attempt as int)
  int capGb = Math.max(baseGb, Math.min(scaledGb, (int)Math.floor(hostGb * 0.85))) // 85% system cap
  return "${capGb} GB"
}

// Helper function to respect user overrides from params.advanced
// Usage: getUserCpus('align_cpus', fallbackClosure)
def getUserCpus = { paramKey, fallbackClosure ->
  def userValue = params.advanced?.get(paramKey)
  return userValue ? { userValue as int } : fallbackClosure
}

def getUserMemory = { paramKey, fallbackClosure ->
  def userValue = params.advanced?.get(paramKey) 
  return userValue ? { userValue } : fallbackClosure
}

def getUserTime = { paramKey, fallbackTime = '24h' ->
  def userValue = params.advanced?.get(paramKey)
  return userValue ?: fallbackTime
}

// ════════════════════════════════════════════════════════════════════════════
/* Process Configuration and Resource Management */
// ════════════════════════════════════════════════════════════════════════════
// This section defines default resource allocation for all processes and
// provides specific overrides for computationally intensive tasks like alignment.
// All resources are dynamically allocated based on available host capabilities.
// Note: Container engines (Docker/Singularity) are handled by profiles.
process {
  // ════════════════════════════════════════════════════════════════════════════
  // Global Process Defaults - Balanced for most tasks
  // ════════════════════════════════════════════════════════════════════════════
  
  // Queue size: Allow multiple tasks to be queued for better resource utilization
  queueSize     = Math.max(16, Math.min(256, (int)(params.host_cpus * 6)))
  
  // Default CPU allocation: 25% of host CPUs per task (enables concurrency)
  cpus          = { fitCpus(0.25d) }
  // Dynamic memory allocation: 15-25% of host memory per task
  memory        = {
    int hostGb   = (params.host_mem as int)
    int perTask  = Math.max(2, (int)Math.floor(hostGb * 0.15))  // 15% baseline
    int capGb    = Math.min(perTask, Math.max(4, (int)Math.floor(hostGb * 0.25)))  // 25% cap
    return "${capGb} GB"
  }
  
  // Time limit: 24 hours (generous for most tasks)
  time          = '24h'
  
  // Use scratch space for temporary files (better performance)
  scratch       = true
  cleanup       = true
  
  // Error handling: Retry on memory/timeout errors, terminate on others
  errorStrategy = { task.exitStatus in [137,143] ? 'retry' : 'terminate' }
  maxRetries    = 1
  maxErrors     = -1
  
  // ════════════════════════════════════════════════════════════════════════════
  // Environment Variables for Optimal Threading
  // ════════════════════════════════════════════════════════════════════════════
  // These variables ensure that bioinformatics tools use the allocated CPU cores
  // efficiently and avoid over-threading which can cause performance degradation.
  beforeScript = '''
    export OMP_NUM_THREADS=${task.cpus}      # OpenMP threading
    export OPENBLAS_NUM_THREADS=${task.cpus} # BLAS library threading
    export MKL_NUM_THREADS=${task.cpus}      # Intel Math Kernel Library
    export BOWTIE2_THREADS=${task.cpus}      # Bowtie2 alignment threads
    export SAMTOOLS_THREADS=${task.cpus}     # Samtools processing threads
    export BEDTOOLS_THREADS=${task.cpus}     # Bedtools analysis threads
  '''

  // ════════════════════════════════════════════════════════════════════════════
  // Process-Specific Resource Allocations
  // ════════════════════════════════════════════════════════════════════════════
  
  // SRA Download: Lightweight process, minimal resources needed
  withName: /(?i).*download_srr.*/ {
    cpus   = getUserCpus('download_cpus', { Math.min(2, params.host_cpus as int) })
    memory = getUserMemory('download_mem', { fitMemory(0.05, task.attempt ?: 1) })
    time   = getUserTime('download_time', '12h')
  }

  // Bowtie2 Index Building: Memory-intensive, moderate threading
  withName: /(?i).*fetch_and_build_index.*/ {
    cpus   = getUserCpus('index_cpus', { fitCpus(0.5d) })
    memory = getUserMemory('index_mem', { fitMemory(0.4, task.attempt ?: 1) })  // Dynamic: 40% of RAM
    time   = getUserTime('index_time', '36h')
  }

  // Alignment (Bowtie2): Most computationally intensive process
  withName: /(?i).*run_alignment.*/ {
    cpus   = getUserCpus('align_cpus', { Math.max(4, fitCpus(0.7d)) })  // Minimum 4 CPUs, up to 70% of host
    memory = getUserMemory('align_mem', { fitMemory(0.7, task.attempt ?: 1) })  // 70% of RAM with scaling
    time   = getUserTime('align_time', '24h')
  }

  // Track Generation (bedGraph/BigWig): Moderate computational load
  withName: /(?i).*generate_tracks.*/ {
    cpus   = getUserCpus('tracks_cpus', { Math.max(2, fitCpus(0.4d)) })
    memory = getUserMemory('tracks_mem', { fitMemory(0.3, task.attempt ?: 1) })
    time   = getUserTime('tracks_time', '12h')
  }

  // Divergent Transcription Detection: CPU-intensive Python analysis
  withName: /(?i).*detect_divergent_tx.*/ {
    cpus         = getUserCpus('divergent_cpus', { Math.max(4, fitCpus(0.5d)) })
    memory       = getUserMemory('divergent_mem', { fitMemory(0.7, task.attempt ?: 1) })
    time         = getUserTime('divergent_time', '12h')
    maxForks     = 1      // Single task to avoid memory contention
    errorStrategy = { task.exitStatus in [137,143] ? 'retry' : 'terminate' }
    maxRetries    = 2     // More retries for complex analysis
    scratch       = true  // Use scratch space for temporary files
  }

  // Track Normalization: Memory-intensive scaling and BigWig conversion
  withName: /(?i).*normalize_tracks.*/ {
    cpus   = getUserCpus('norm_cpus', { Math.max(4, fitCpus(0.5d)) })
    memory = getUserMemory('norm_mem', { fitMemory(0.6, task.attempt ?: 1) })
    time   = getUserTime('norm_time', '12h')
  }

  // Light/Medium Processing Steps: Moderate computational requirements
  withName: /(?i)(prepare_input|call_functional_regions|calculate_pol2_metrics|qc_pol2_tracktx|collect_counts).*/ {
    cpus   = { Math.max(2, fitCpus(0.3d)) }  // 30% of CPUs for moderate processing
    memory = { fitMemory(0.15, task.attempt ?: 1) }  // 15% of RAM with scaling
    time   = '8h'  // Moderate timeout
  }

  // Lightweight Steps: Minimal resource requirements
  withName: /(?i)(download_gtf|generate_reports|combine_reports|generate_readme).*/ {
    cpus   = { 1 }  // Single CPU for simple tasks
    memory = { fitMemory(0.05, task.attempt ?: 1) }  // 5% of RAM with scaling
    time   = '2h'  // Short timeout
  }
}

// ════════════════════════════════════════════════════════════════════════════
/* Configuration Notes and Usage */
// ════════════════════════════════════════════════════════════════════════════
//
// DYNAMIC RESOURCE ALLOCATION SYSTEM
// ═══════════════════════════════════
// This configuration automatically detects and optimally distributes system resources
// across all pipeline processes. No manual resource configuration is needed!
//
// 🔍 System Detection:
//   • CPU cores: Runtime.runtime.availableProcessors() or NXF_HOST_CPUS
//   • Memory: JMX getTotalPhysicalMemorySize() or NXF_HOST_MEM 
//   • Auto-scales from laptops (4 cores) to HPC nodes (100+ cores)
//
// 📊 Resource Distribution Strategy:
//   • High-intensity: Alignment (70% CPU/RAM), Divergent Detection (50% CPU, 70% RAM)
//   • Medium-intensity: Track Generation (40% CPU, 30% RAM), Normalization (50% CPU, 60% RAM)
//   • Lightweight: Reports, Downloads (1 CPU, 5% RAM)
//
// 🎛️ User Overrides (Optional):
//   • Set params.advanced.align_cpus = 8 to force alignment to use 8 CPUs
//   • Set params.advanced.norm_mem = '16 GB' to force normalization memory
//   • Available keys: align_*, tracks_*, norm_*, divergent_*, index_*, etc.
//
// 🔄 Auto-scaling Features:
//   • Memory scales with retry attempts (task.attempt)
//   • 85% RAM cap prevents system overload
//   • Minimum CPU/memory guarantees prevent resource starvation
//   • Queue size scales with CPU count for optimal throughput
//
// 💡 Examples:
//   • 4-core laptop: alignment gets 3 CPUs, tracks get 2 CPUs
//   • 32-core server: alignment gets 22 CPUs, tracks get 13 CPUs  
//   • 128 GB RAM: alignment gets ~90 GB, tracks get ~38 GB
//
// ⚙️ Manual System Override:
//   export NXF_HOST_CPUS=16    # Force 16 CPU detection
//   export NXF_HOST_MEM=64     # Force 64 GB RAM detection
//
// Note: Trace/timeline/report/dag configurations are handled globally in nextflow.config
// to avoid duplication and ensure consistent output locations (results/trace/*).
// Engine-specific flags (e.g., Docker memory limits) belong in profiles.
