// ════════════════════════════════════════════════════════════════════════════
// TrackTx Performance Optimization Config
// ════════════════════════════════════════════════════════════════════════════
//
// PURPOSE:
//   Optimize pipeline for external storage (USB SSD, NAS, network drives)
//   where I/O bottlenecks significantly slow down execution.
//
// USE THIS WHEN:
//   - Running from external storage (USB SSD, NAS, network drive)
//   - Pipeline feels unreasonably slow on external drives
//   - Heavy I/O activity visible (disk I/O is the bottleneck)
//
// HOW TO USE:
//   Via run_pipeline.sh:
//     ./run_pipeline.sh --fast
//
//   Or manually:
//     nextflow run main.nf -profile docker -params-file params.yaml \
//       -work-dir ~/tmp/tracktx_work -c performance.config
//
// EXPECTED IMPROVEMENT:
//   2-3x faster execution on external drives
//   Reduces I/O bottlenecks by avoiding scratch space copying
//
// KEY OPTIMIZATIONS:
//   1. Disable scratch space (no file copying to/from temp directories)
//   2. Increase parallelism for better throughput on slow I/O
//   3. Optimize executor queue size for better scheduling
//
// ════════════════════════════════════════════════════════════════════════════

// ── System Detection (must redeclare as functions aren't inherited) ────────
def detectCpus() {
  def cpus = Runtime.runtime.availableProcessors()
  def envCpus = System.getenv('NXF_HOST_CPUS')
  return envCpus ? (envCpus as Integer) : cpus
}

def getSystemCpus() { 
  return Math.max(1, detectCpus()) 
}

// ── Performance-Optimized Fork Calculations ────────────────────────────────
// These build on the adaptive system but increase parallelism for better
// throughput on slow I/O devices

def performanceAlignmentForks() {
  def total = getSystemCpus()
  if (total < 8) return 2  // Allow some parallelism even on small systems
  // More aggressive parallelism for external drives (1.5x adaptive)
  return Math.max(2, (int)(total / 4))
}

def performancePrepareForks() {
  def total = getSystemCpus()
  if (total < 6) return 2
  return Math.max(2, (int)(total / 3))  // More parallel than default
}

def performanceTracksForks() {
  def total = getSystemCpus()
  if (total < 6) return 2
  return Math.max(2, (int)(total / 2))  // Very parallel for I/O-bound work
}

def performanceNormForks() {
  def total = getSystemCpus()
  if (total < 10) return 2
  return Math.max(2, Math.min(4, (int)(total / 4)))  // Allow more parallelism
}

def performanceLightweightForks() {
  def total = getSystemCpus()
  // Increase lightweight task parallelism significantly
  return Math.max(6, (int)(total * 1.5))
}

def performanceAnalysisForks() {
  def total = getSystemCpus()
  return Math.max(3, (int)(total / 2))  // More parallel analysis
}

process {
  // ── CRITICAL: Disable Scratch Space ───────────────────────────────────────
  // On external drives, scratch copying is extremely slow (2-10x slower)
  // Keeping files in work directory eliminates this bottleneck
  // This is the #1 performance improvement for external storage
  scratch = false
  
  // ── Keep Cleanup Enabled ──────────────────────────────────────────────────
  // Remove task directories after successful completion to save space
  cleanup = true
  
  // ── Global Max Forks (Adaptive) ───────────────────────────────────────────
  // Increase parallelism based on system size, but respect CPU limits
  // This improves throughput on slow I/O by keeping more tasks active
  maxForks = Math.max(8, Math.min(16, (int)(getSystemCpus() * 1.2)))
  
  // ── Process-Specific Optimizations ────────────────────────────────────────
  
  // CPU-Intensive: Alignment (still needs careful balancing)
  withName: /(?i).*run_alignment.*/ {
    // Use adaptive forks but slightly increased for better I/O utilization
    maxForks = performanceAlignmentForks()
    // Keep scratch disabled (critical for external drives)
    scratch  = false
  }
  
  // I/O-Intensive: Track Generation (benefits from more parallelism)
  withName: /(?i).*generate_tracks.*/ {
    maxForks = performanceTracksForks()
    scratch  = false
  }
  
  // I/O-Intensive: Normalization (can run more in parallel on slow I/O)
  withName: /(?i).*normalize_tracks.*/ {
    maxForks = performanceNormForks()
    // Disable scratch even for intensive processes (external drive optimization)
    scratch  = false
  }
  
  // I/O-Intensive: Divergent Detection (can parallelize more)
  withName: /(?i).*detect_divergent_tx.*/ {
    maxForks = performanceAnalysisForks()
    scratch  = false
  }
  
  // Medium-Intensity: Input Preparation
  withName: /(?i).*prepare_input.*/ {
    maxForks = performancePrepareForks()
    scratch  = false
  }
  
  // Analysis Processes
  withName: /(?i).*calculate_pol_metrics.*/ {
    maxForks = performanceAnalysisForks()
    scratch  = false
  }
  
  withName: /(?i).*call_functional_regions.*/ {
    maxForks = performanceAnalysisForks()
    scratch  = false
  }
  
  // Lightweight: Can run many in parallel
  withName: /(?i)(collect_counts|qc_pol_tracktx|summarize_pol_metrics).*/ {
    maxForks = performanceLightweightForks()
    scratch  = false
  }
  
  // Lightweight: Reports, Downloads
  withName: /(?i).*download_(gtf|srr).*/ {
    maxForks = Math.max(2, (int)(getSystemCpus() / 3))
    scratch  = false
  }
  
  withName: /(?i)(generate_reports|combine_reports).*/ {
    maxForks = performanceLightweightForks()
    scratch  = false
  }
  
  // Index Building: Keep sequential (resource-intensive)
  withName: /(?i).*fetch_and_build_index.*/ {
    maxForks = 1  // Index building: one at a time (as in main config)
    scratch  = false
  }
}

// ── Executor Optimizations ──────────────────────────────────────────────────
executor {
  // Increase queue size for better scheduling with increased parallelism
  // This helps when many tasks are waiting for I/O
  queueSize = 200
}

// ── Docker Optimizations (if using Docker) ──────────────────────────────────
docker {
  // Remove containers immediately after task completion
  // Reduces disk usage and improves cleanup speed
  remove = true
}

// ── Notes ───────────────────────────────────────────────────────────────────
//
// TRADE-OFFS:
//   ✓ Faster execution on external drives (2-3x speedup)
//   ✓ Better utilization of slow I/O
//   ✓ No scratch copying overhead
//   ✗ Work directory will be larger (no intermediate file cleanup)
//   ✗ May use more disk space during execution
//
// WHEN NOT TO USE:
//   - Running from fast internal SSD (scratch space may be beneficial)
//   - Limited disk space (scratch cleanup helps)
//   - Very large datasets where scratch cleanup is essential
//
// RECOMMENDED WORKFLOW:
//   1. Use -work-dir to point work directory to fast internal storage:
//      -work-dir ~/tmp/tracktx_work (macOS/Linux)
//      -work-dir C:\temp\tracktx_work (Windows)
//   2. Keep output_dir on external drive (final results only)
//   3. Pipeline will be fast, results will be on external drive
